{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dictG =  731\n",
      "*****Inverted Index and Boolean retrival *********\n",
      "Enter your query - ( center and welcome )\n",
      "line 124, listC=  [['', 0], ['center', '6', '1', '3', '2', '5'], ['welcome', '2']]\n",
      "line 130, queryG=  ['(', 'center', 'and', 'welcome', ')']\n",
      "line 133, listC:  [['center', '6', '1', '3', '2', '5'], ['welcome', '2']]\n",
      "The Docs against the query are - \n",
      "['2']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This program does the following work - \n",
    "\n",
    "1. Builds an inverted index from data files in /data folder and \n",
    "saves the index in index.docx file in the same workng directory\n",
    "\n",
    "2. Returns document IDs for Boolean queries.\n",
    "Query format - ( term1 op term2 )\n",
    "Operators - and, or, not (all lower case)\n",
    "\n",
    "Stemmer used - Porter Stemmer from NLTK package\n",
    "Lemmantizer - NLTK package based Lemmantizer\n",
    "Tokenizer - NLTK based Tokenizer\n",
    "\n",
    "Short Flow of events - \n",
    "\n",
    "1. From the given path of input docs, generate the Inverted index and store it in a data structure.\n",
    "Also store it in a file index.docx.\n",
    "\n",
    "2. Take the input query from the user in format - ( term op term ).\n",
    "\n",
    "3. Show the list of doc ID which contain the term\n",
    "\n",
    "Test Queries and output - test.txt\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "''' Imports'''\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from docx import Document\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import glob\n",
    "import errno\n",
    "import collections\n",
    "import os\n",
    "from collections.abc import Sequence\n",
    "\n",
    "'''\n",
    "__download the stop words__\n",
    "__Uncomment and run if the terms aren't present__\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "'''\n",
    "#initializing data structures\n",
    "dictG=[['',0]] #Final Dictionary\n",
    "queryG=[] #Query\n",
    "listC=[['',0]] \n",
    "docsG=[] #Docslist\n",
    "\n",
    "#Proccess docs for index building\n",
    "def buildIndex(fileName, docID):\n",
    "\n",
    "    words = set() #set to store words from each doc\n",
    "    \n",
    "    #from each doc, append words into raw words list\n",
    "    document=Document(fileName)\n",
    "    for p in document.paragraphs:\n",
    "        listT=p.text.split()\n",
    "        for word in listT:\n",
    "            words.add(word)\n",
    "    \n",
    "    #Removing Stop words\n",
    "    stop_words=list(stopwords.words('english'))    \n",
    "    filteredQ=[w for w in list(words) if not w in stop_words]\n",
    "#     print('***************** filtered sentence after removing stop words*****************')\n",
    "#     print(filteredQ)\n",
    "    \n",
    "    #Lemmentizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmaList=[]\n",
    "    for word in filteredQ:\n",
    "        lemmaList.append(lemmatizer.lemmatize(word))\n",
    "#     print('*************** After lemmatization**********')\n",
    "#     print(lemmaList)\n",
    "    \n",
    "    # if a word is already present in dictionary, append this doc Id to list of doc ids in which the word in present\n",
    "    # if the word in already present in the doc, ie, same doc has the word two times, then don't add doc Id\n",
    "    # else add [word, docid] to dictionary\n",
    "    \n",
    "    flagD=0\n",
    "    flagDuplicateDoc=0\n",
    "    for word in lemmaList:\n",
    "        for element in dictG:\n",
    "            if(element[0]==word):\n",
    "                flagD=1\n",
    "                for x in element[1:]:\n",
    "                    if(x==docID):\n",
    "                        flagDuplicateDoc=1\n",
    "                        break\n",
    "                if(flagDuplicateDoc==0):\n",
    "                    element.append(docID)\n",
    "                if(flagDuplicateDoc==1):\n",
    "                    flagDuplicateDoc=0\n",
    "        if(flagD==0):\n",
    "            dictG.append([word,docID])\n",
    "        if(flagD==1):\n",
    "            flagD=0\n",
    "\n",
    "'''Proccesses input Boolean query'''\n",
    "def processQuery():\n",
    "    global queryG\n",
    "    flagQ=0\n",
    "    queryRaw=input('Enter your query - ')\n",
    "    queryT=queryRaw.split()\n",
    "    #lemmeantize the query\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in queryT:\n",
    "        queryG.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    obtainTermsFromDictionary()\n",
    "    getRequiredDocs()\n",
    "\n",
    "''' Obtains terms from Index - Returns - list of \n",
    "terms+docID ['term',id,id...]'''\n",
    "def obtainTermsFromDictionary():\n",
    "    global dictG\n",
    "    global listC\n",
    "    listC=[['',0]]\n",
    "    for word in queryG:\n",
    "        for element in dictG:\n",
    "            if(word==element[0]):\n",
    "                listC.append(element)\n",
    "    print(\"line 124, listC= \", listC)\n",
    "    listC.pop(0)\n",
    "\n",
    "'''Gets the associated doc IDs, by infix evaluation of list of doc Ids using stack'''\n",
    "def getRequiredDocs():\n",
    "    \n",
    "    print(\"line 130, queryG= \", queryG) #given the current query\n",
    "    ListA = list() #temporary list of docs for boolean query\n",
    "    termCount = 0 #current termcount\n",
    "    print(\"line 133, listC: \", listC)\n",
    "    templist = listC[termCount][1:] #doc list from 1st term    \n",
    "    stack = list(); #stack to manage the query\n",
    "    \n",
    "    while len(queryG) > 0:\n",
    "        \n",
    "        c = queryG.pop(0)\n",
    "\n",
    "        if isinstance(c, Sequence):\n",
    "            if c in ['and','or','not']: \n",
    "                stack.append(c)\n",
    "            else:\n",
    "                for e in listC:\n",
    "                    if e[0] == c:\n",
    "                        stack.append(listC[listC.index(e)][1:])\n",
    "        \n",
    "        if ')' in c:\n",
    "            \n",
    "            num2 = stack.pop()\n",
    "            op = stack.pop()\n",
    "            num1 = stack.pop()\n",
    "            \n",
    "            if op == \"and\":\n",
    "                stack.append(And(num1,num2))\n",
    "            if op == \"or\":\n",
    "                stack.append(Or(num1,num2))\n",
    "            if op == \"not\":\n",
    "                stack.append(Not(num1,num2))\n",
    "          \n",
    "    print('The Docs against the query are - ')\n",
    "    print(stack[0])\n",
    "\n",
    "'''Logical Not operation - Args(list,list)\n",
    "returns - list'''\n",
    "def Not(list1,list2):\n",
    "    \n",
    "    ListA = list()\n",
    "    if(len(list1) >= len(list2)):\n",
    "        for x in list1:\n",
    "            if x not in list2:\n",
    "                ListA.append(x)\n",
    "    elif(len(list2) >= len(list1)):            \n",
    "        for x in list2:\n",
    "             if x not in list1:\n",
    "                ListA.append(x)\n",
    "    return ListA\n",
    "\n",
    "'''Logical Or operation - Args(list,list)\n",
    "returns - list'''\n",
    "def Or(list1,list2):\n",
    "    \n",
    "    ListA = list()\n",
    "    for x in list1:\n",
    "        ListA.append(x)\n",
    "    for y in list2:\n",
    "        if(y not in ListA):\n",
    "            ListA.append(y)\n",
    "    \n",
    "    return list(set(ListA))\n",
    "\n",
    "'''Logical And operation - Args(list,list)\n",
    "returns - list'''\n",
    "def And(list1,list2):\n",
    "    \n",
    "    ListA = list()\n",
    "    for x in list1:\n",
    "        for y in list2:\n",
    "            if(x == y):\n",
    "                ListA.append(x)\n",
    "    \n",
    "    return ListA\n",
    "\n",
    "'''Final running of code'''\n",
    "#Replace path according to your working directory structure\n",
    "directory = os.path.join(os.path.dirname('C:\\\\Users\\\\akshaya\\\\Documents\\\\Code'), 'data')\n",
    "path = os.path.join(directory, '*.docx')\n",
    "\n",
    "files=glob.glob(path)\n",
    "\n",
    "for name in files:\n",
    "    docID = name.split('.')[0][-1]\n",
    "    buildIndex(name,docID)\n",
    "\n",
    "\n",
    "# __Code to create the index file__\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Index', 0)\n",
    "\n",
    "for element in dictG:\n",
    "    document.add_paragraph(str(element))\n",
    "\n",
    "#save index to a file\n",
    "document.save('index.docx')\n",
    "\n",
    "print(\"len of dictG = \", len(dictG))\n",
    "\n",
    "\n",
    "print('*****Inverted Index and Boolean retrival *********')\n",
    "#Receive and Proccess the Query\n",
    "processQuery()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
